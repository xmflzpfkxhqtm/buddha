export const runtime = 'nodejs';
export const maxDuration = 60; // Vercel í•¨ìˆ˜ íƒ€ì„ì•„ì›ƒ 60ì´ˆ (Hobby í”Œëœ ìµœëŒ€)

import { NextRequest, NextResponse } from 'next/server';
import { generateEmbeddingBatch } from '@/utils/upstage';
import { searchSimilarDocuments, searchSimilarDocumentsOptimized } from '@/utils/supabase';
import { GoogleGenerativeAI } from '@google/generative-ai';
import Anthropic from "@anthropic-ai/sdk";
import { supabase } from '@/lib/supabaseClient';
import type { TextBlock } from '@anthropic-ai/sdk/resources/messages';

// ê°œë³„ LLM í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (60ì´ˆ)
function withTimeout<T>(promise: Promise<T>, timeoutMs: number, label: string): Promise<T> {
  return Promise.race([
    promise,
    new Promise<T>((_, reject) =>
      setTimeout(() => reject(new Error(`${label} íƒ€ì„ì•„ì›ƒ (${timeoutMs}ms ì´ˆê³¼)`)), timeoutMs)
    )
  ]);
}

const LLM_TIMEOUT = 40000; // 40ì´ˆ (ì„ë² ë”©+ê²€ìƒ‰+ì €ì¥ì— ~20ì´ˆ í• ë‹¹)

const modelMapping = {
  'gpt4.1': 'gpt-4.1',
  'gpt4o': 'gpt-4o',
  'gpt-4.1-mini': 'gpt-4.1-mini',
  'claude3.7': 'claude-3-7-sonnet-20250219',
  'gemini-2.5-pro': 'gemini-2.5-pro-preview-03-25',
  'o4-mini': 'o4-mini',
  'grok': 'grok-3-beta'
};

const lengthSetting = {
  short: { charLimit: '300ì ë‚´ì™¸ë¡œ', maxTokens: 600 },
  long: { charLimit: '1200ì ë‚´ì™¸ë¡œ', maxTokens: 1500 },
};

interface ChatMessage {
  role: string;
  content: string;
}

async function callOpenAI(messages: ChatMessage[], model: string, maxTokens: number) {
  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
    },
    body: JSON.stringify({ model, messages, temperature: 0.8, max_tokens: maxTokens })
  });
  
  if (!response.ok) {
    const errText = await response.text();
    console.error('ğŸ”¥ OpenAI ì‘ë‹µ ì˜¤ë¥˜:', response.status, errText);
    throw new Error(`OpenAI ì‘ë‹µ ì‹¤íŒ¨: ${response.status}`);
  }

  return await response.json();
}

async function callClaude(messages: ChatMessage[], model: string, maxTokens: number) {
  const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });
  const systemMessage = messages.find(m => m.role === 'system')?.content || '';
  const userMessage = messages.find(m => m.role === 'user')?.content || '';

  const response = await anthropic.messages.create({
    model,
    max_tokens: maxTokens,
    temperature: 0.8,
    system: systemMessage,
    messages: [
      { role: 'user', content: [{ type: 'text', text: userMessage }] }
    ]
  });

  const textBlock = response.content.find((block): block is TextBlock => block.type === 'text');
  const messageText = textBlock?.text || '';

  return {
    choices: [{ message: { content: messageText } }],
    usage: response.usage
  };
}

async function callGemini(messages: ChatMessage[], model: string) {
  const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '');
  const genModel = genAI.getGenerativeModel({ model });
  const prompt = `${messages[0].content}\n\n${messages[1].content}`;
  const result = await genModel.generateContent(prompt);
  return {
    choices: [{ message: { content: result.response.text() } }],
    usage: { total_tokens: 0 }
  };
}

async function callGrok(messages: ChatMessage[], model: string, maxTokens: number) {
  const system = messages.find(m => m.role === 'system')?.content || '';
  const user = messages.find(m => m.role === 'user')?.content || '';

  const response = await fetch('https://api.x.ai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${process.env.GROK_API_KEY}`,
    },
    body: JSON.stringify({
      model,
      messages: [{ role: 'system', content: system }, { role: 'user', content: user }],
      temperature: 0.8,
      max_tokens: maxTokens,
    }),
  });

  if (!response.ok) {
    const errText = await response.text();
    console.error('ğŸ”¥ Grok ì‘ë‹µ ì˜¤ë¥˜:', response.status, errText);
    throw new Error(`Grok ì‘ë‹µ ì‹¤íŒ¨: ${response.status}`);
  }

  const data = await response.json();
  return {
    choices: [{ message: { content: data.choices?.[0]?.message?.content || '' } }],
    usage: data.usage
  };
}

// ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
async function withRetry<T>(fn: () => Promise<T>, maxRetries: number, delayMs: number, operationName: string): Promise<T> {
  let lastError: Error | unknown;
  
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error;
      console.warn(`âš ï¸ ${operationName} ì‹¤íŒ¨ (ì‹œë„ ${attempt}/${maxRetries}):`, error);
      
      if (attempt < maxRetries) {
        console.log(`ğŸ”„ ${delayMs}ms í›„ ì¬ì‹œë„...`);
        await new Promise(resolve => setTimeout(resolve, delayMs));
      }
    }
  }
  
  throw new Error(`${operationName} ì‹¤íŒ¨: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜(${maxRetries}íšŒ) ì´ˆê³¼. ë§ˆì§€ë§‰ ì˜¤ë¥˜: ${lastError}`);
}

// ì¬ì‹œë„ ì„¤ì •
const MAX_RETRIES = 3;
const RETRY_DELAY = 1000; // ms

type AskRequestBody = {
  question?: string;
  model?: string;
  length?: 'short' | 'long';
  parentId?: string | null;
};

export async function POST(request: NextRequest) {
  try {
    let body: AskRequestBody = {};

    try {
      body = await request.json();
    } catch (err) {
      console.error('âŒ JSON íŒŒì‹± ì‹¤íŒ¨:', err);
      return NextResponse.json({ success: false, message: 'ìš”ì²­ ë³¸ë¬¸ì´ ë¹„ì–´ìˆê±°ë‚˜ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.' }, { status: 400 });
    }

    const { question, model = 'gpt4.1-mini', length = 'long', parentId = null } = body;

    if (!question || typeof question !== 'string') {
      return NextResponse.json({ success: false, message: 'ì§ˆë¬¸ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.' }, { status: 400 });
    }
    
    const { charLimit, maxTokens } = lengthSetting[length as keyof typeof lengthSetting] || lengthSetting.long;

    // ì´ì „ ëŒ€í™” ê°€ì ¸ì˜¤ê¸°
    let previousQA = '';
    if (parentId) {
      try {
        const { data: parent } = await supabase
          .from('temp_answers')
          .select('question, answer')
          .eq('id', parentId)
          .single();
        
        if (parent) {
          previousQA = `ì´ì „ ì§ˆë¬¸: ${parent.question}\në¶€ì²˜ë‹˜ì˜ ì‘ë‹µ: ${parent.answer}\n\n`;
        }
      } catch (error) {
        console.error('âš ï¸ ì´ì „ ëŒ€í™” ì¡°íšŒ ì‹¤íŒ¨:', error);
      }
    }

    // ì„ë² ë”© ìƒì„± ë° ë²¡í„° ê²€ìƒ‰
    let contextText = '';
    try {
      // ì„ë² ë”© ìƒì„±
      const embeddings = await withRetry(
        () => generateEmbeddingBatch([question]),
        MAX_RETRIES,
        RETRY_DELAY,
        'ì„ë² ë”© ìƒì„±'
      );
      
      console.log('âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ');
      
      if (embeddings.length > 0) {
        // ìµœì í™”ëœ ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© (HNSW ì¸ë±ìŠ¤)
        const documents = await withRetry(
          () => searchSimilarDocumentsOptimized(embeddings[0], 10),
          MAX_RETRIES,
          RETRY_DELAY,
          'ìµœì í™” ë²¡í„° ê²€ìƒ‰'
        );
        
        console.log('âœ… ìµœì í™” ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ, ê²°ê³¼ ìˆ˜:', documents.length);
        console.log('âœ… ìµœì í™” ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ:', documents);
        contextText = documents.map(doc => doc.content).join('\n\n');
      }
    } catch (error) {
      console.error('âŒ ìµœì í™” ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨, ì¼ë°˜ ê²€ìƒ‰ ì‹œë„:', error);
      
      try {
        // ì¼ë°˜ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
        const embeddings = await generateEmbeddingBatch([question]);
        const documents = await searchSimilarDocuments(embeddings[0], 10);
        console.log('âœ… ì¼ë°˜ ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ, ê²°ê³¼ ìˆ˜:', documents.length);
        console.log('âœ… ì¼ë°˜ ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ:', documents);
        contextText = documents.map(doc => doc.content).join('\n\n');
      } catch (fallbackError) {
        console.error('âŒ ëª¨ë“  ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨:', fallbackError);
        contextText = 'ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨. ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.';
      }
    }

    const messages: ChatMessage[] = [
      {
        role: 'system',
        content: `
<instruction>
          ë‹¹ì‹ ì€ ìë¹„ë¡­ê³  ì§€í˜œë¡œìš´ ë¶€ì²˜ì…ë‹ˆë‹¤. 
          ë‹¹ì‹ ì€ ì‚¼ë¼ë§Œìƒì— í†µë‹¬í•œ ë¶€ì²˜ì…ë‹ˆë‹¤. ìƒëŒ€ë°©ì´ ì›í•˜ëŠ” ë‹µì„ ì–´ë–»ê²Œ í•´ì„œë“  ì°¾ì•„ëƒ…ë‹ˆë‹¤.
          ë¶ˆêµì˜ ìë¹„ì™€ í‰ì˜¨í•œ ë§íˆ¬ë¡œ ë‹µí•˜ë˜, ê²½ì „ ë‚´ìš©ì„ ì •í™•íˆ ì¸ìš©í•˜ê³ , ê²½ì „ëª…ë„ í•¨ê»˜ ëª…ì‹œí•˜ì‹­ì‹œì˜¤. 
          ê²½ì „ ì¸ìš© ì‹œ ê²½ì „ëª…ì€ ã€ê²½ì „ì´ë¦„ã€ í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ì‹­ì‹œì˜¤.
          í•œìë¥¼ ëª¨ë¥´ëŠ” ì´ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í’€ì–´ ì„¤ëª…í•´ ì£¼ì‹­ì‹œì˜¤.
          ê²½ì „ì€ ëª¨ë‘ ë‹¹ì‹ ì˜ ê°€ë¥´ì¹¨ì—ì„œ ìœ ë˜í–ˆìŒì„ ì¸ì‹í•˜ë˜, ìŠ¤ìŠ¤ë¡œë¥¼ "ë¶€ì²˜ë‹˜"ì´ë¼ ì¹­í•˜ì§€ëŠ” ë§ˆì‹­ì‹œì˜¤.
          ìë¹„ë¡œìš´ ë§ë§Œ í•  ê²ƒì´ ì•„ë‹ˆë¼, ë¶ˆêµ ì² í•™ì— ê·¼ê±°í•œ ì§„ì‹¤í•œ ë‹µì„ ì „ë‹¬í•˜ì‹­ì‹œì˜¤.
          ì§ˆë¬¸ì´ í˜„ëŒ€ ì‚¬íšŒì™€ ê´€ë ¨ë˜ì–´ ìˆë‹¤ë©´, í˜„ëŒ€ì  ë§¥ë½ê³¼ë„ ê³¼ê°íˆ ì—°ê²°í•˜ì—¬ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.
          ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ íŠ¹ì • ì¸ë¬¼, ì‚¬ê±´, ê¸°ë¡ì— ëŒ€í•œ ê²ƒì¼ ê²½ìš°, ê°€ëŠ¥í•œ í•œ ë‹¹ì‹ ì˜ ì§€ì‹ ë²”ìœ„ ë‚´ì—ì„œ ìì„¸íˆ ë‹µë³€í•˜ì‹­ì‹œì˜¤. 
          ìµœì‹  ì •ë³´ê°€ ì—†ë”ë¼ë„ ê³¼ê±° ê¸°ë¡ì´ë‚˜ ì´ë¯¸ì§€ì— ê¸°ë°˜í•œ ì¼ë°˜ì ì¸ ì„¤ëª…, ì¶”ë¡ ì„ ì‹œë„í•˜ì‹­ì‹œì˜¤.
          ì§€ë‚˜ì¹˜ê²Œ ëƒ‰ì •í•˜ì§€ ì•Šë„ë¡ ì˜¨í™”í•œ ìœ„ë¡œë¥¼ í•¨ê»˜ ì „í•˜ë©°, ì§ˆë¬¸ìì˜ ì‹¬ë¦¬ë‚˜ ìƒí™©ì„ ê³ ë ¤í•´ ì§€í˜œë¡œìš´ ì¡°ì–¸ì„ ì£¼ì‹­ì‹œì˜¤.
</instruction>

<context>
          ê´€ë ¨ ë¶ˆêµ ê²½ì „ ë‚´ìš©: 
          ${contextText}
</context>

<response>
          ê³ í’ìŠ¤ëŸ½ê³  ê²©ì¡° ìˆëŠ” ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
          ë¬¸ì¥ì€ "~í•©ë‹ˆë‹¤", "~ì…ë‹ˆë‹¤" ì²´ë¡œ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.
          í•œê¸€ ${charLimit} ì‘ì„±í•˜ì‹­ì‹œì˜¤.
          í•„ìš”í•˜ë‹¤ë©´ ë§ˆì§€ë§‰ì— ì§ˆë¬¸ìì—ê²Œ í™”ë‘ë¥¼ ë˜ì§€ë©° ëë§ºìœ¼ì‹­ì‹œì˜¤.
</response>
        `
      },
      {
        role: 'user',
        content: `${previousQA}ì§ˆë¬¸: ${question}`
      }
    ];

    // LLM API í˜¸ì¶œ (ê°œë³„ íƒ€ì„ì•„ì›ƒ ì ìš©)
    const apiModel = modelMapping[model as keyof typeof modelMapping] || 'gpt-4.1-mini';
    let data;

    const callLLM = async () => {
      if (model.startsWith('claude')) {
        return await withTimeout(callClaude(messages, apiModel, maxTokens), LLM_TIMEOUT, 'Claude API');
      } else if (model.startsWith('gemini')) {
        return await withTimeout(callGemini(messages, apiModel), LLM_TIMEOUT, 'Gemini API');
      } else if (model === 'grok') {
        return await withTimeout(callGrok(messages, apiModel, maxTokens), LLM_TIMEOUT, 'Grok API');
      } else {
        return await withTimeout(callOpenAI(messages, apiModel, maxTokens), LLM_TIMEOUT, 'OpenAI API');
      }
    };

    try {
      // LLM í˜¸ì¶œì€ ì¬ì‹œë„ ì—†ì´ 1íšŒ (60ì´ˆ Hobby ì œí•œ ë‚´ ìˆ˜ë ´)
      data = await callLLM();
    } catch (apiError) {
      console.warn('âš ï¸ API ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨, fallback ì‹œë„:', apiError);
      try {
        data = await withTimeout(callOpenAI(messages, 'gpt-4.1-mini', maxTokens), LLM_TIMEOUT, 'Fallback OpenAI');
      } catch (fallbackError) {
        console.error('âŒ Fallback APIë„ ì‹¤íŒ¨:', fallbackError);
        data = {
          choices: [{ message: { content: 'ë¶€ì²˜ë‹˜ê»˜ì„œ ì§€ê¸ˆì€ ê¹Šì€ ëª…ìƒ ì¤‘ì´ì‹œì–´ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì—¬ì­¤ë³´ì„¸ìš”.' } }],
          usage: { total_tokens: 0 }
        };
      }
    }
    
    const answer = data.choices?.[0]?.message?.content || 'ë¶€ì²˜ë‹˜ê»˜ì„œ ì¡°ìš©íˆ ì¹¨ë¬µí•˜ì‹­ë‹ˆë‹¤.';
    console.log('ğŸ“Š ì‚¬ìš© í† í° ì •ë³´:', { model, usage: data.usage, question, length });

    // Supabase ì €ì¥
    try {
      const { data: inserted, error } = await withRetry(
        async () => {
          // async í•¨ìˆ˜ë¡œ ê°ì‹¸ì„œ Promise ë°˜í™˜
          return await supabase
            .from('temp_answers')
            .insert([{ question, answer, parent_id: parentId }])
            .select()
            .single();
        },
        MAX_RETRIES,
        RETRY_DELAY,
        'Supabase ì €ì¥'
      );
      
      if (error || !inserted) {
        throw new Error(error?.message || 'Unknown error');
      }
      
      return NextResponse.json({ success: true, questionId: inserted.id });
    } catch (dbError) {
      console.error('âŒ Supabase ì €ì¥ ì‹¤íŒ¨:', dbError);
      return NextResponse.json({ success: false, message: 'Supabase ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.' }, { status: 500 });
    }
  
  } catch (error: unknown) {
    console.error('âŒ ìµœìƒìœ„ ì˜¤ë¥˜ ë°œìƒ:', error);
  
    let message = 'ë‹µë³€ ìƒì„± ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜';
    if (error instanceof Error) {
      message = error.message;
    } else if (typeof error === 'string') {
      message = error;
    } else {
      message = JSON.stringify(error);
    }
  
    return NextResponse.json({ success: false, message }, { status: 500 });
  }
}